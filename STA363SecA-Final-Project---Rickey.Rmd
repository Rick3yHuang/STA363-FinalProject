---
title: "STA363SecAFinalProject"
author: "Rickey Huang"
date: "5/9/2021"
output: 
  pdf_document:
  toc: yes
  number_sections: ture
citation_package: biblatex
bibliography: Final Project citation.bib
csl: modern-language-association-6th-edition-note.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\newpage
\tableofcontents
\listoffigures
\listoftables
\newpage

```{r, message=FALSE}
# Library every required packages here
#Package for visualizing missing data
# Run the next line of code if the "mice" package is not installed before
#install.packages("mice")
library(mice)
# Packages for ggplot2
library(rlang)
library(ggplot2)
# Packages for fitting and plotting trees
library(rpart)
library(rattle)
library(rpart.plot)
# Packages for fitting forests
# Run the next line of code if the "randomForest" package is not installed before
#install.packages("randomForest")
library(randomForest)
```

## Abstract

## Section 1: Data and Motivation

### Section 1.1: Goal of the Analysis

This is a Project exploring the cutomers' satisfaction of the US Ariline. The main goal of the analysis is both association and prediction. To be more specific, this project wants to understand which variables and how these variables affect the satisfaction of passengers who take a flight of US Airlnes. The data set used in this project is the result from a survey taken by passengers of US Airline [@noauthor_airline_nodate]. In the survey, basic information of the passengers, like gender, age, and the flight information, like customer type, class, departure/arrival time were collected. Other than these objective data, some subjective scores are also asked in the survey, like ease of online booking, gate location, food and drink, and so on. All these scores are scaled in the range from $1$ to $5$.

### Section 1.2: The Data Set

```{r}
# Loading the data set
Airline <- read.csv("~/Desktop/2021Spring/STA-363/Projects/Final Project/STA363-FinalProject/Data/Airline Passenger Satisfaction/Airline Satisfaction.csv")
```

```{r, results="hide"}
dim(Airline)
```

As taking a closer look at the data set, the Airline data set has $36728$ observations and $23$ variables. Among these variables, the response variable is *"satisfaction"* which is a categorical variable with two levels "satisfied" and "dissatisfiaction". In the rest of the $35$ variables, $4$ of them are numerical type data, which are *"Age"*, *"Flight.Distance"*, *"Departure.Delay.in.Minutes"*, and *"Arrival.Delay.in.Minutes"*, while the rest are all categorical variable variables. *"Age"* represents the age of the passengers who take the flight. *"Flight.Distance"* is the distance of the flight. *"Departure.Delay.in.Minutes"* is the time of delay for departure in minutes, and *"Arrival.Delay.in.Minutes"* is the time of dealy for arrival time in minutes. *"Gender"* of the passengers is a binary categorical variable. *"Type.ofTravel"* is also a binary categorical variable which has two levels "Personal Travel" or "Bussiness Travel". *"Class"* means the travel class in the plane in three levels "Bussiness", "Eco", and "Eco Plus". The variables like *"Inflight.wifi"* and *"Ease.of.Online.booking"* are categorical variables evaluating variables scored by passengers from $1$ to $5$, where $1$ means least satisfied, while $5$ means most satisfied. The original dataset can be accessed using the url: https://kaggle.com/teejmahal20/airline-passenger-satisfaction.

## Section 2: Data Cleaning

### Section 2.1: Missing Data

```{r, result = "hide"}
#summary(Airline)
# Find out how many data are missing
#md.pattern(Airline, rotate.name = TRUE, plot = TRUE)
```

After checking any missing data in the data set, the Airline data is found to be completely observed, which means there is no missing data. Hence no imputation are required here.

### Section 2.2: Editing the Variables

```{r}
# Create a new variable which is the same as the satisfaction, but using 1 to represent satisfied 0 as neutral or unsatisfies
Airline$satisfaction.num[Airline$satisfaction == "satisfied"] <- 1
Airline$satisfaction.num[Airline$satisfaction == "neutral or dissatisfied"] <- 0
```

Since the response variable *"satisfaction"* is a binary categorical variable, and for the convenience of analysis with methods like logistic regression, a new variable *"satisfactin.num"* is created based on the original variable which include the same information as *"satisfaction"* but is recorded in a different way. "$1$" in *"satisfaction.num"* means "satisfied" in *"satisfaction"*, while "$0$" represents "neutral or dissatisfied".

## Section 3: Method 1: 

```{r}
op <- par(mfrow = c(2,3))
boxplot(Age ~ satisfaction, xlab = "Satisfaction", ylab = "Age", data = Airline)
boxplot(Flight.Distance ~ satisfaction, xlab = "Cleanliness", ylab = "Age", data = Airline)
plot(x = as.factor(Airline$Gender), y = as.factor(Airline$satisfaction), xlab = "Gender", ylab = "Satisfaction")
plot(x = as.factor(Airline$Customer.Type), y = as.factor(Airline$satisfaction), xlab = "Customer Type", ylab = "Satisfaction")
plot(x = as.factor(Airline$Type.of.Travel), y = as.factor(Airline$satisfaction), xlab = "Type of Travel", ylab = "Satisfaction")
plot(x = as.factor(Airline$Class), y = as.factor(Airline$satisfaction), xlab = "Class", ylab = "Satisfaction")
par(op)
```


```{r}
emplogit <- function(x, y, binsize = NULL, ci = FALSE, probit = FALSE,prob = FALSE, main = NULL, xlab = "", ylab = "", lowess.in = FALSE){
  # x         vector with values of the independent variable
  # y         vector of binary responses
  # binsize   integer value specifying bin size (optional)
  # ci        logical value indicating whether to plot approximate
  #           confidence intervals (not supported as of 02/08/2015)
  # probit    logical value indicating whether to plot probits instead
  #           of logits
  # prob      logical value indicating whether to plot probabilities
  #           without transforming
  #
  # the rest are the familiar plotting options
  
  if (length(x) != length(y))
    stop("x and y lengths differ")
  if (any(y < 0 | y > 1))
    stop("y not between 0 and 1")
  if (length(x) < 100 & is.null(binsize))
    stop("Less than 100 observations: specify binsize manually")
  
  if (is.null(binsize)) binsize = min(round(length(x)/10), 50)
  
  if (probit){
    link = qnorm
    if (is.null(main)) main = "Empirical probits"
  } else {
    link = function(x) log(x/(1-x))
    if (is.null(main)) main = "Empirical logits"
  }
  
  sort = order(x)
  x = x[sort]
  y = y[sort]
  a = seq(1, length(x), by=binsize)
  b = c(a[-1] - 1, length(x))
  
  prob = xmean = ns = rep(0, length(a)) # ns is for CIs
  for (i in 1:length(a)){
    range = (a[i]):(b[i])
    prob[i] = mean(y[range])
    xmean[i] = mean(x[range])
    ns[i] = b[i] - a[i] + 1 # for CI 
  }
  
  extreme = (prob == 1 | prob == 0)
  prob[prob == 0] = min(prob[!extreme])
  prob[prob == 1] = max(prob[!extreme])
  
  g = link(prob) # logits (or probits if probit == TRUE)
  
  linear.fit = lm(g[!extreme] ~ xmean[!extreme])
  b0 = linear.fit$coef[1]
  b1 = linear.fit$coef[2]
  
  loess.fit = loess(g[!extreme] ~ xmean[!extreme])
  
  plot(xmean, g, main=main, xlab=xlab, ylab=ylab)
  abline(b0,b1)
  if(lowess.in ==TRUE){
  lines(loess.fit$x, loess.fit$fitted, lwd=2, lty=2)
  }
}
op <- par(mfrow = c(1,2))
emplogit(x = Airline$Age, y = Airline$satisfaction.num, binsize = 20, xlab = "Age ", ylab = "Empirical Logit")
emplogit(x = Airline$Flight.Distance, y = Airline$satisfaction.num, binsize = 20, xlab = "Flight Distance", ylab = "Empirical Logit")
emplogit(x = Airline$Departure.Delay.in.Minutes, y = Airline$satisfaction.num, binsize = 20, xlab = "Flight Distance", ylab = "Empirical Logit")
emplogit(x = Airline$Arrival.Delay.in.Minutes, y = Airline$satisfaction.num, binsize = 20, xlab = "Flight Distance", ylab = "Empirical Logit")
par(op)
```

## Section 4: Method 2: Tree & Forest Models

### Section 3.1: Introduction

#### Section 4.1.1 Reasons for considering another method

Even though the logistic regression model with BSS in the method $1$ does the shrinkage and gives a regression line that reveals the association and makes prediction, the model is restricted by this single regression line. A transformation of the variables or a change in the form of the regression line would change the model a lot. Therefore, the tree model and forest model should be considered sicne they are not sensitive to the form of regression lines, while to tree model and forest model are not useing all of the variables in the model for the prediction, they also do the selection as the BSS does.

#### Section 4.1.2: How could the tree and forest model answer the reseach quesiton
Since the response variable *"satisfaction"* is a categorical data, and the goal of the project is to understand which variables affect the satisfaction (association) and how this variables affect the satisfaction (prediction). Both the classification tree model and classifcation random forest model are created in the method 1. Since the tree model has a clear and direct visualization to understand the association between variables, which can do the association job. The classification random forest model using bootstrap samples with random subset of variables to train the model and make the prediction, so it could be a technique to sovle the prediction requirement. A combination of the tree model and forest model will be counted as the method to solve the research question here. 

### Section 4.2: Data Visualization (EDA)

Since the tree method and the forest model don't need to decide the type of the model like regression models, in which the form of the regression line need to be decide first after analyzing the correlation of the variables, the detailed relationship among variable are not necessary here for the tree and forest model. Instead, the distribution of the response variable is explored.

In order to explore the distribution of the response variable, a bar chart Figure \ref{fig:responseDistribution} is created, because the response variable is a categorical variable. From the bar chart we can see that there are $20990$ observations are at the "neutral or dissatisfied" level, while $15738$ rows are at the "satisfied" level. Since there is not a level that has an extremely low frequency comparing to the other, the tree and forest model are safe to be used here.

```{r responseDistribution, fig.cap="\\label{fig:responseDistribution}Distribution of the response variable", fig.asp=0.6}
# Visualizing the distribution of the response variable
ggplot(Airline, aes(x = satisfaction)) + 
  geom_bar(width=0.7, color = 'darkblue', fill = 'steelblue') + 
  geom_text(stat = 'count', aes(label=..count..), vjust=-0.3, size=3.5)
```

### Section 4.3: Method

#### Section 4.3.1: Tree Model

##### Section 4.3.1.1: The Full Tree Model

First, a full tree model with all variables in the data set is trained. The fullTree model has a Root Node Error (RNE) of $0.4285$, and the RNE in the classification tree model represents the Classification Error Rate (CER) of the root node. the the test CER for each split can be calculated using the Formula \ref{eq:testCER}, which is the RNE times the percent change from RNE by each split. Since we want a low error rate, we want the to have a the test CER that is only a small portion of the RNE. The cp plot is created in Figure \ref{fig:cpPlot}. it depicts how the change in the cp affects the test CER, and the number of splits are shown on the top of the plot. Hence, a number of split between $1$ and $18$ would create a comparatively small CER and with a appropriate number of splits. 

\begin{equation}\label{eq:testCER}
test\; CER = RNE \times xerror
\end{equation}

```{r}
# Set seed
set.seed(2021)
Airline.original <- subset(Airline, select = - satisfaction.num)
# train the fullTree model using all features in the data
fullTree <- rpart(satisfaction ~ ., method = "class", data = Airline.original,control = rpart.control(minsplit = 10,
minsize = 5, mindev=0, cp =0))
# Show the RNE of the root node
#printcp(fullTree)
# Compute the test CER for the full model
```

```{r cpPlot, fig.cap="\\label{fig:cpPlot}cp plot for fullTree", fig.asp = 0.6}
# Compute the test MSE for the pruned tree
#48872*0.1650205
# Plot the relationship between xerror and the cp values
plotcp(fullTree)
```

##### Section 4.3.1.2: Methodology for Grwoing and Pruning the Tree Model

In order to grow the classification tree model, the Gini Index of trees are computed and compared, and the fullTree model is created by minimizing the Gini Index, since a smaller Gini Index implies a more stable model. The Gini Index could be calculated using the Formula \ref{eq:GiniIndex}, where $\left\vert T \right\vert$ is the number of leafs in the tree, and $G({Leaf}_l)$ is the Impurity Score at a certain leaf, which can be computed using the Formula \ref{eq:Impurity}. In this formula, n represents the number of levels in the response variable.

\begin{equation}\label{eq:GiniIndex}
Gini\; Index = \sum_{l = 1}^{\left\vert T \right\vert} \dfrac{n_l}{n}G({Leaf}_l)
\end{equation}

\begin{equation}\label{eq:Impurity}
G({Leaf}_{i}) = 1 - \sum_{j=1}^{m}\hat{p}_{(Y = {level}_{j},{Leaf}_{i})}^{2}
\end{equation}

In order to prune the tree, the a penalty term is added to the classification tree model with an optimal tuning parameter $\alpha$. Then, the pruned tree model would be created by minimizing the penalized Gini Index.

##### Secction 4.3.1.3: Pruning the fullTree Model to create the Tree Model

Since the whole cp table is too long, only part of the cp table of fullTree is shown in the Table \ref{tab:cpTable}. To find an optimal $\alpha$, the percent change from the RNE is compared, and a $14$-split tree is chosen here since with further split, the percent change from the RNE is only $0.003$ or smaller, so the further splits is not worthy. Hence, we have the optimal $\alpha = 0.185792$.

\begin{table}[]
\centering
\caption{cp table for the fullTree (excerpts)}
\label{tab:cpTable}
\begin{tabular}{|c|c|c|c|}
\hline
CP         & nsplit & rel error & xerror  \\ \hline
7.8155e-03 & 8      & 0.238658  & 0.22010 \\ \hline
5.8775e-03 & 12     & 0.197547  & 0.20142 \\ \hline
3.5583e-03 & 14     & 0.185792  & 0.18630 \\ \hline
3.1770e-03 & 15     & 0.182234  & 0.18300 \\ \hline
3.1135e-03 & 17     & 0.175880  & 0.17950 \\ \hline
\end{tabular}
\end{table}

The Pruned Tree is shown in the Figure \ref{fig:TreeV1}. This is a detailed visualization of the Tree model since the portion of data in each leaf is shown in this figure. Also a simplified visualization that is much easier to be explained is shown in the Figure \ref{fig:TreeV2}. The darker a certain color is, the prediction is more stable.


```{r TreeV1, fig.cap="\\label{fig:TreeV1}The Detailed Visualization for Tree"}
# storing the optimal alpha in x
x <- fullTree$cptable[9]
# Pruning to get the Tree model
Tree <- prune(fullTree, cp = x)
# Visualization 1 for the Tree model
rpart.plot(Tree)
```

```{r TreeV2, fig.cap = "\\label{fig:TreeV2}The Concise Visualization for Tree"}
# Visualization 2 for the Tree model
prp(Tree, box.palette = "RdYlGn", cex = 0.7)
```

#### Section 4.3.2: Forest Model

##### Section 4.3.2.1: Methodology for Growing the Random Forest

The random forest is created by creating bootstrap samples, and using these samples to trained the trees. When the trees are trained, only $\left\lfloor \sqrt{p} \right\rfloor$ of the total number of features ($p$) are used, so that the random forest model representative of the population is high.

##### Section 4.3.2.2: Fitting the Random Forest Model

```{r, results='hide'}
Airline.new <- subset(Airline, select = - satisfaction)
# Set seed
set.seed(2021)
# Train the Random Forest Model
RdForest <- randomForest(satisfaction.num ~ ., data = Airline.new, mtry = sqrt(22), importance = TRUE, ntree = 100, compete = FALSE)
```

$1000$ bootstrap samples are made to train the Random Forest model (RdForest). Also, since there are $22$ features in the data, $\left\lfloor \sqrt{p} \right\rfloor = 4$ features are randomly chosen to fit the tree model with each boostrap sample.

## Conclusions

\newpage
## Works Cited Page


